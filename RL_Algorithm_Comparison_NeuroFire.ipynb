{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1acd72",
   "metadata": {},
   "source": [
    "# ü§ñ RL Algorithm Comparison: DQN vs PPO vs A2C\n",
    "## Enhanced NeuroFire Project Analysis\n",
    "\n",
    "**Objective**: Compare three state-of-the-art Deep Reinforcement Learning algorithms for autonomous firefighter drone control, with comprehensive performance analysis and visualizations.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. **Environment Setup & Dependencies**\n",
    "2. **Custom Environment Implementation** \n",
    "3. **DQN Agent Architecture & Training**\n",
    "4. **PPO Agent Architecture & Training**\n",
    "5. **A2C Agent Architecture & Training**\n",
    "6. **Training Orchestration & Monitoring**\n",
    "7. **Performance Evaluation & Metrics**\n",
    "8. **Comparative Visualization & Analysis**\n",
    "9. **Interactive Simulations**\n",
    "10. **Final Results & Recommendations**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Goals\n",
    "- Train and compare 3 RL algorithms on NeuroFire environment\n",
    "- Visualize learning curves and convergence behavior\n",
    "- Analyze strengths/weaknesses of each approach\n",
    "- Generate actionable recommendations for algorithm selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "690770a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using device: cpu\n",
      "‚úÖ All dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# SECTION 1: Environment Setup & Dependencies\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import time\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Using device: {DEVICE}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Plotting styles\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe69ab6",
   "metadata": {},
   "source": [
    "# SECTION 2: Custom NeuroFire Environment\n",
    "\n",
    "A simplified fire-fighting environment where an AI agent learns to navigate and extinguish fires in a grid-based forest.\n",
    "\n",
    "**State**: Direction to nearest fire, obstacles around agent (11 inputs)\n",
    "**Actions**: Move straight, turn right, turn left (3 discrete actions)  \n",
    "**Reward**: +10 for extinguishing fire, -10 for hitting obstacle, -1 per step\n",
    "**Goal**: Maximize total reward by efficiently navigating and extinguishing fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28f44512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NeuroFireSimplified environment...\n",
      "‚úÖ Initial state shape: (11,)\n",
      "‚úÖ State sample: [0. 0. 0. 0. 0.]\n",
      "   Step 1: action=0, reward=-0.01, done=False\n",
      "   Step 2: action=1, reward=-0.01, done=False\n",
      "   Step 3: action=0, reward=-0.01, done=False\n",
      "   Step 4: action=1, reward=-0.01, done=False\n",
      "   Step 5: action=1, reward=-0.01, done=False\n"
     ]
    }
   ],
   "source": [
    "# Simplified NeuroFire Environment for Algorithm Comparison\n",
    "class NeuroFireSimplified:\n",
    "    \"\"\"Simplified Fire Environment for RL Algorithm Comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=20, fire_density=0.1):\n",
    "        self.grid_size = grid_size\n",
    "        self.fire_density = fire_density\n",
    "        self.state_size = 11  # Sensor inputs\n",
    "        self.action_size = 3  # Straight, Right, Left\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        # Agent position\n",
    "        self.agent_x = self.grid_size // 2\n",
    "        self.agent_y = self.grid_size // 2\n",
    "        self.agent_direction = 0  # 0=Up, 1=Right, 2=Down, 3=Left\n",
    "        \n",
    "        # Fire positions (random)\n",
    "        self.fires = set()\n",
    "        for _ in range(int(self.grid_size**2 * self.fire_density)):\n",
    "            self.fires.add((np.random.randint(0, self.grid_size),\n",
    "                          np.random.randint(0, self.grid_size)))\n",
    "        \n",
    "        # Obstacles (walls at boundaries + some internal)\n",
    "        self.obstacles = set()\n",
    "        for i in range(self.grid_size):\n",
    "            self.obstacles.add((i, 0))\n",
    "            self.obstacles.add((i, self.grid_size-1))\n",
    "            self.obstacles.add((0, i))\n",
    "            self.obstacles.add((self.grid_size-1, i))\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.max_steps = 200\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get sensor state (11-dimensional observation)\"\"\"\n",
    "        state = np.zeros(self.state_size, dtype=np.float32)\n",
    "        \n",
    "        # Direction encoding\n",
    "        state[0] = self.agent_direction / 4.0\n",
    "        \n",
    "        # Obstacle sensors (8 directions)\n",
    "        directions = [(-1, 0), (-1, 1), (0, 1), (1, 1),\n",
    "                     (1, 0), (1, -1), (0, -1), (-1, -1)]\n",
    "        for i, (dx, dy) in enumerate(directions):\n",
    "            nx, ny = self.agent_x + dx, self.agent_y + dy\n",
    "            state[i+1] = 1.0 if (nx, ny) in self.obstacles else 0.0\n",
    "        \n",
    "        # Fire detection (nearest fire direction)\n",
    "        if self.fires:\n",
    "            fire_distances = [(((self.agent_x - fx)**2 + (self.agent_y - fy)**2)**0.5, fx, fy)\n",
    "                            for fx, fy in self.fires]\n",
    "            _, nearest_fx, nearest_fy = min(fire_distances)\n",
    "            \n",
    "            # Fire bearing (-1 to 1)\n",
    "            fire_dy = nearest_fy - self.agent_y\n",
    "            fire_dx = nearest_fx - self.agent_x\n",
    "            state[10] = np.clip(fire_dx / (self.grid_size / 2), -1, 1)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action: 0=Straight, 1=Right, 2=Left\"\"\"\n",
    "        self.steps += 1\n",
    "        reward = 0\n",
    "        \n",
    "        # Update direction and position\n",
    "        if action == 1:  # Turn right\n",
    "            self.agent_direction = (self.agent_direction + 1) % 4\n",
    "        elif action == 2:  # Turn left\n",
    "            self.agent_direction = (self.agent_direction - 1) % 4\n",
    "        \n",
    "        # Move forward\n",
    "        dx = [0, 1, 0, -1][self.agent_direction]\n",
    "        dy = [-1, 0, 1, 0][self.agent_direction]\n",
    "        \n",
    "        new_x = self.agent_x + dx\n",
    "        new_y = self.agent_y + dy\n",
    "        \n",
    "        # Check collision with obstacles\n",
    "        if (new_x, new_y) in self.obstacles:\n",
    "            reward -= 10  # Penalty for hitting wall\n",
    "        else:\n",
    "            self.agent_x = new_x\n",
    "            self.agent_y = new_y\n",
    "        \n",
    "        # Check if fire extinguished\n",
    "        if (self.agent_x, self.agent_y) in self.fires:\n",
    "            self.fires.remove((self.agent_x, self.agent_y))\n",
    "            reward += 10  # Reward for extinguishing fire\n",
    "        \n",
    "        # Step penalty\n",
    "        reward -= 0.01\n",
    "        \n",
    "        done = self.steps >= self.max_steps or len(self.fires) == 0\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Simple text rendering\"\"\"\n",
    "        print(\"Grid (Agent: @, Fire: F, Wall: #):\")\n",
    "        for y in range(self.grid_size):\n",
    "            row = \"\"\n",
    "            for x in range(self.grid_size):\n",
    "                if (x, y) == (self.agent_x, self.agent_y):\n",
    "                    row += \"@\"\n",
    "                elif (x, y) in self.fires:\n",
    "                    row += \"F\"\n",
    "                elif (x, y) in self.obstacles:\n",
    "                    row += \"#\"\n",
    "                else:\n",
    "                    row += \".\"\n",
    "            print(row)\n",
    "\n",
    "# Test environment\n",
    "print(\"Testing NeuroFireSimplified environment...\")\n",
    "test_env = NeuroFireSimplified()\n",
    "state = test_env.reset()\n",
    "print(f\"‚úÖ Initial state shape: {state.shape}\")\n",
    "print(f\"‚úÖ State sample: {state[:5]}\")\n",
    "\n",
    "for i in range(5):\n",
    "    action = np.random.randint(0, 3)\n",
    "    state, reward, done = test_env.step(action)\n",
    "    print(f\"   Step {i+1}: action={action}, reward={reward:.2f}, done={done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dbca18",
   "metadata": {},
   "source": [
    "# SECTION 3: DQN Agent (Deep Q-Network)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Learning Type**: Off-policy (learns from exploration data)\n",
    "- **Architecture**: Value-based (estimates Q-values)\n",
    "- **Experience Replay**: Stores and samples past transitions\n",
    "- **Target Network**: Separate network for stable Q-target\n",
    "- **Epsilon-Greedy**: Exploration-exploitation trade-off\n",
    "\n",
    "**Advantages:**\n",
    "‚úì Sample efficient (off-policy learning)  \n",
    "‚úì Stable training with target network  \n",
    "‚úì Works with discrete actions  \n",
    "\n",
    "**Disadvantages:**\n",
    "‚úó Can overestimate Q-values  \n",
    "‚úó Requires careful hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb2f4553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DQN Agent implemented\n"
     ]
    }
   ],
   "source": [
    "# DQN Implementation\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience Replay Buffer\"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.Transition = namedtuple('Transition', \n",
    "                                    ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.buffer.append(self.Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return self.Transition(*zip(*batch))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Network Agent\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=1e-4, gamma=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = DQNNetwork(state_size, action_size).to(DEVICE)\n",
    "        self.target_net = DQNNetwork(state_size, action_size).to(DEVICE)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Optimizer and memory\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.memory = ReplayBuffer()\n",
    "        \n",
    "        # Metrics\n",
    "        self.losses = []\n",
    "        self.rewards = []\n",
    "        self.q_values = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, self.action_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "            q_values = self.policy_net(state_t)\n",
    "            return q_values.argmax(1).item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in memory\"\"\"\n",
    "        self.memory.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train_step(self, batch_size=64):\n",
    "        \"\"\"Training step\"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return 0\n",
    "        \n",
    "        batch = self.memory.sample(batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(batch.state)).to(DEVICE)\n",
    "        actions = torch.LongTensor(np.array(batch.action)).unsqueeze(1).to(DEVICE)\n",
    "        rewards = torch.FloatTensor(np.array(batch.reward)).to(DEVICE)\n",
    "        next_states = torch.FloatTensor(np.array(batch.next_state)).to(DEVICE)\n",
    "        dones = torch.FloatTensor(np.array(batch.done)).to(DEVICE)\n",
    "        \n",
    "        # Current Q values\n",
    "        q_values = self.policy_net(states).gather(1, actions).squeeze()\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "        \n",
    "        # Loss\n",
    "        loss = F.mse_loss(q_values, target_q_values)\n",
    "        \n",
    "        # Backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if len(self.memory) % 500 == 0:\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        return loss.item()\n",
    "\n",
    "print(\"‚úÖ DQN Agent implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba46828",
   "metadata": {},
   "source": [
    "# SECTION 4: PPO Agent (Proximal Policy Optimization)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Learning Type**: On-policy (learns from recent data)\n",
    "- **Architecture**: Actor-Critic (policy + value)\n",
    "- **Clipped Objective**: Prevents extreme policy updates\n",
    "- **GAE**: Generalized Advantage Estimation for variance reduction\n",
    "- **Multiple Epochs**: Updates on same batch multiple times\n",
    "\n",
    "**Advantages:**\n",
    "‚úì More stable than policy gradient methods  \n",
    "‚úì Better sample efficiency than vanilla A3C  \n",
    "‚úì Easy to parallelize  \n",
    "\n",
    "**Disadvantages:**\n",
    "‚úó More complex hyperparameters  \n",
    "‚úó Higher computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1008f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PPO Agent implemented\n"
     ]
    }
   ],
   "source": [
    "# PPO Implementation\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"Actor-Critic network for PPO\"\"\"\n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shared = self.shared(x)\n",
    "        return self.actor(shared), self.critic(shared)\n",
    "\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"Proximal Policy Optimization Agent\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=3e-4, gamma=0.99, \n",
    "                 gae_lambda=0.95, clip_ratio=0.2, epochs=4):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.policy = ActorCritic(state_size, action_size).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        \n",
    "        # Metrics\n",
    "        self.losses = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action and compute log probability\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, value = self.policy(state_t)\n",
    "        \n",
    "        if training:\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            self.states.append(state)\n",
    "            self.actions.append(action.item())\n",
    "            self.values.append(value.item())\n",
    "            self.log_probs.append(log_prob)\n",
    "            \n",
    "            return action.item()\n",
    "        else:\n",
    "            return probs.argmax(1).item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def compute_advantages(self, next_value=0):\n",
    "        \"\"\"Compute GAE advantages\"\"\"\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for t in reversed(range(len(self.rewards))):\n",
    "            if t == len(self.rewards) - 1:\n",
    "                next_val = next_value\n",
    "            else:\n",
    "                next_val = self.values[t + 1]\n",
    "            \n",
    "            delta = self.rewards[t] + self.gamma * next_val - self.values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        return torch.FloatTensor(advantages).to(DEVICE)\n",
    "    \n",
    "    def train(self, next_state, done):\n",
    "        \"\"\"Update policy using PPO\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Compute next value for bootstrapping\n",
    "        if done:\n",
    "            next_value = 0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                _, v = self.policy(torch.FloatTensor(next_state).unsqueeze(0).to(DEVICE))\n",
    "                next_value = v.item()\n",
    "        \n",
    "        # Compute advantages and returns\n",
    "        advantages = self.compute_advantages(next_value)\n",
    "        returns = advantages + torch.FloatTensor(self.values).to(DEVICE)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states)).to(DEVICE)\n",
    "        actions = torch.LongTensor(np.array([self.actions])).squeeze().to(DEVICE)\n",
    "        old_log_probs = torch.stack(self.log_probs).to(DEVICE)\n",
    "        \n",
    "        # PPO update\n",
    "        total_loss = 0\n",
    "        for _ in range(self.epochs):\n",
    "            probs, values = self.policy(states)\n",
    "            dist = Categorical(probs)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # Policy loss\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(values.squeeze(), returns)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        self.losses.append(total_loss / self.epochs)\n",
    "        \n",
    "        # Clear storage\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        self.values, self.log_probs = [], []\n",
    "        \n",
    "        return total_loss / self.epochs\n",
    "\n",
    "print(\"‚úÖ PPO Agent implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40571ec0",
   "metadata": {},
   "source": [
    "# SECTION 5: A2C Agent (Advantage Actor-Critic)\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Learning Type**: On-policy (learns from recent data)\n",
    "- **Architecture**: Actor-Critic (policy + value)\n",
    "- **Synchronous**: No parallel workers (unlike A3C)\n",
    "- **TD Error**: Uses temporal difference for advantage\n",
    "- **Entropy Bonus**: Encourages exploration\n",
    "\n",
    "**Advantages:**\n",
    "‚úì Simple to implement  \n",
    "‚úì Faster convergence than vanilla PG  \n",
    "‚úì Lower variance than reinforce  \n",
    "\n",
    "**Disadvantages:**\n",
    "‚úó Less stable than PPO  \n",
    "‚úó Sensitive to hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c855725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ A2C Agent implemented\n"
     ]
    }
   ],
   "source": [
    "# A2C Implementation\n",
    "class A2CAgent:\n",
    "    \"\"\"Advantage Actor-Critic Agent\"\"\"\n",
    "    def __init__(self, state_size, action_size, lr=3e-4, gamma=0.99,\n",
    "                 value_coef=0.5, entropy_coef=0.01):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        self.policy = ActorCritic(state_size, action_size).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Metrics\n",
    "        self.losses = []\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        \"\"\"Select action using policy\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, value = self.policy(state_t)\n",
    "        \n",
    "        if training:\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            return action.item(), log_prob, value\n",
    "        else:\n",
    "            return probs.argmax(1).item(), None, None\n",
    "    \n",
    "    def train(self, state, action, reward, next_state, done, log_prob):\n",
    "        \"\"\"A2C update\"\"\"\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "        next_state_t = torch.FloatTensor(next_state).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, v_current = self.policy(state_t)\n",
    "            _, v_next = self.policy(next_state_t)\n",
    "        \n",
    "        # TD target\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + self.gamma * v_next.item()\n",
    "        \n",
    "        # Get fresh values and probs for loss computation\n",
    "        probs, v_pred = self.policy(state_t)\n",
    "        dist = Categorical(probs)\n",
    "        \n",
    "        # Advantage\n",
    "        advantage = td_target - v_pred.item()\n",
    "        \n",
    "        # Losses\n",
    "        actor_loss = -log_prob * advantage\n",
    "        critic_loss = (v_pred.squeeze() - td_target) ** 2\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = actor_loss + self.value_coef * critic_loss - self.entropy_coef * entropy\n",
    "        \n",
    "        # Update\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        return loss.item()\n",
    "\n",
    "print(\"‚úÖ A2C Agent implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9258c0",
   "metadata": {},
   "source": [
    "# SECTION 6: Training Orchestration\n",
    "\n",
    "Now we'll train all three agents on the NeuroFire environment and monitor their progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nüöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ \n",
      "STARTING AGENT TRAINING\n",
      "üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ üöÄ \n",
      "\n",
      "üìä Environment Info:\n",
      "   State Size: 11\n",
      "   Action Size: 3\n",
      "   Max Steps per Episode: 200\n",
      "\n",
      "ü§ñ Initializing Agents...\n",
      "‚úÖ All agents initialized\n",
      "\n",
      "============================================================\n",
      "üéØ Training DQN Agent\n",
      "============================================================\n",
      "Episode  50 | Reward:   -2.00 | Mean(100):   57.60 | Best:  168.00\n",
      "Episode 100 | Reward:   48.00 | Mean(100):   65.90 | Best:  218.00\n",
      "Episode 150 | Reward:    8.00 | Mean(100):   84.80 | Best:  238.00\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_agent(agent, agent_name, episodes=200):\n",
    "    \"\"\"Train an agent on the NeuroFire environment\"\"\"\n",
    "    env = NeuroFireSimplified()\n",
    "    \n",
    "    episode_rewards = []\n",
    "    mean_rewards = []\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ Training {agent_name} Agent\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done and step < env.max_steps:\n",
    "            # Select action\n",
    "            if agent_name == \"DQN\":\n",
    "                action = agent.select_action(state, training=True)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                agent.train_step()\n",
    "                \n",
    "            elif agent_name == \"PPO\":\n",
    "                action = agent.select_action(state, training=True)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.store_reward(reward)\n",
    "                \n",
    "            elif agent_name == \"A2C\":\n",
    "                action, log_prob, value = agent.select_action(state, training=True)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.train(state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        \n",
    "        # PPO training at end of episode\n",
    "        if agent_name == \"PPO\":\n",
    "            agent.train(state, done)\n",
    "        \n",
    "        # Store metrics\n",
    "        episode_rewards.append(episode_reward)\n",
    "        mean_reward = np.mean(episode_rewards[-100:])\n",
    "        mean_rewards.append(mean_reward)\n",
    "        \n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "        \n",
    "        # Progress print\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode+1:3d} | Reward: {episode_reward:7.2f} | \"\n",
    "                  f\"Mean(100): {mean_reward:7.2f} | Best: {best_reward:7.2f}\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"‚úÖ Training Complete!\\n\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'mean_rewards': mean_rewards,\n",
    "        'losses': agent.losses,\n",
    "        'best_reward': best_reward\n",
    "    }\n",
    "\n",
    "# Train all three agents\n",
    "print(\"\\\\n\" + \"üöÄ \"*30)\n",
    "print(\"STARTING AGENT TRAINING\")\n",
    "print(\"üöÄ \"*30)\n",
    "\n",
    "# Environment info\n",
    "env = NeuroFireSimplified()\n",
    "print(f\"\\nüìä Environment Info:\")\n",
    "print(f\"   State Size: {env.state_size}\")\n",
    "print(f\"   Action Size: {env.action_size}\")\n",
    "print(f\"   Max Steps per Episode: {env.max_steps}\")\n",
    "\n",
    "# Initialize agents\n",
    "print(f\"\\nü§ñ Initializing Agents...\")\n",
    "dqn_agent = DQNAgent(env.state_size, env.action_size)\n",
    "ppo_agent = PPOAgent(env.state_size, env.action_size)\n",
    "a2c_agent = A2CAgent(env.state_size, env.action_size)\n",
    "print(f\"‚úÖ All agents initialized\")\n",
    "\n",
    "# Train agents\n",
    "num_episodes = 200\n",
    "dqn_results = train_agent(dqn_agent, \"DQN\", episodes=num_episodes)\n",
    "ppo_results = train_agent(ppo_agent, \"PPO\", episodes=num_episodes)\n",
    "a2c_results = train_agent(a2c_agent, \"A2C\", episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d1d57a",
   "metadata": {},
   "source": [
    "# SECTION 7: Performance Evaluation & Metrics\n",
    "\n",
    "Evaluate the trained agents on the NeuroFire environment without exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e67e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_agent(agent, agent_name, num_eval_episodes=20):\n",
    "    \"\"\"Evaluate agent performance without training\"\"\"\n",
    "    env = NeuroFireSimplified()\n",
    "    \n",
    "    eval_rewards = []\n",
    "    eval_fires_extinguished = []\n",
    "    \n",
    "    for _ in range(num_eval_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        fires_start = len(env.fires)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if agent_name == \"DQN\":\n",
    "                action = agent.select_action(state, training=False)\n",
    "            elif agent_name == \"PPO\":\n",
    "                action = agent.select_action(state, training=False)\n",
    "            elif agent_name == \"A2C\":\n",
    "                action, _, _ = agent.select_action(state, training=False)\n",
    "            \n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        \n",
    "        fires_extinguished = fires_start - len(env.fires)\n",
    "        eval_rewards.append(episode_reward)\n",
    "        eval_fires_extinguished.append(fires_extinguished)\n",
    "    \n",
    "    return {\n",
    "        'rewards': eval_rewards,\n",
    "        'fires_extinguished': eval_fires_extinguished,\n",
    "        'mean_reward': np.mean(eval_rewards),\n",
    "        'std_reward': np.std(eval_rewards),\n",
    "        'mean_fires': np.mean(eval_fires_extinguished)\n",
    "    }\n",
    "\n",
    "# Evaluate all agents\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATING AGENTS (NO TRAINING)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dqn_eval = evaluate_agent(dqn_agent, \"DQN\", num_eval_episodes=20)\n",
    "ppo_eval = evaluate_agent(ppo_agent, \"PPO\", num_eval_episodes=20)\n",
    "a2c_eval = evaluate_agent(a2c_agent, \"A2C\", num_eval_episodes=20)\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation Complete\\n\")\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"{'Agent':<10} {'Mean Reward':>15} {'Std Dev':>15} {'Avg Fires':>15}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'DQN':<10} {dqn_eval['mean_reward']:>15.2f} {dqn_eval['std_reward']:>15.2f} {dqn_eval['mean_fires']:>15.2f}\")\n",
    "print(f\"{'PPO':<10} {ppo_eval['mean_reward']:>15.2f} {ppo_eval['std_reward']:>15.2f} {ppo_eval['mean_fires']:>15.2f}\")\n",
    "print(f\"{'A2C':<10} {a2c_eval['mean_reward']:>15.2f} {a2c_eval['std_reward']:>15.2f} {a2c_eval['mean_fires']:>15.2f}\")\n",
    "print(\"-\"*55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d6150",
   "metadata": {},
   "source": [
    "# SECTION 8: Comprehensive Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727fae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison visualizations\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "colors = {'DQN': '#2E86AB', 'PPO': '#A23B72', 'A2C': '#F18F01'}\n",
    "\n",
    "# 1. Learning Curves (Smoothed)\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "window = 20\n",
    "for name, results in [('DQN', dqn_results), ('PPO', ppo_results), ('A2C', a2c_results)]:\n",
    "    smoothed = np.convolve(results['mean_rewards'], np.ones(window)/window, mode='valid')\n",
    "    ax1.plot(smoothed, label=name, linewidth=2.5, color=colors[name])\n",
    "ax1.set_xlabel('Episode', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Mean Reward (100-ep window)', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('üéØ Learning Curves Comparison', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Best Reward Achieved\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "names = ['DQN', 'PPO', 'A2C']\n",
    "best_rewards = [dqn_results['best_reward'], ppo_results['best_reward'], a2c_results['best_reward']]\n",
    "bars = ax2.bar(names, best_rewards, color=[colors[n] for n in names], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_ylabel('Best Reward', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('üèÜ Best Rewards', fontsize=13, fontweight='bold')\n",
    "for bar, val in zip(bars, best_rewards):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 3. Training Loss\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "for name, results in [('DQN', dqn_results), ('PPO', ppo_results), ('A2C', a2c_results)]:\n",
    "    if results['losses']:\n",
    "        ax3.plot(results['losses'][:300], label=name, linewidth=1.5, alpha=0.8, color=colors[name])\n",
    "ax3.set_xlabel('Training Step', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Loss', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('üìâ Training Loss', fontsize=13, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Evaluation Rewards (Boxplot)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "eval_data = [dqn_eval['rewards'], ppo_eval['rewards'], a2c_eval['rewards']]\n",
    "bp = ax4.boxplot(eval_data, labels=['DQN', 'PPO', 'A2C'], patch_artist=True)\n",
    "for patch, name in zip(bp['boxes'], names):\n",
    "    patch.set_facecolor(colors[name])\n",
    "    patch.set_alpha(0.7)\n",
    "ax4.set_ylabel('Episode Reward', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('üì¶ Reward Distribution', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Mean Evaluation Performance\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "mean_eval = [dqn_eval['mean_reward'], ppo_eval['mean_reward'], a2c_eval['mean_reward']]\n",
    "std_eval = [dqn_eval['std_reward'], ppo_eval['std_reward'], a2c_eval['std_reward']]\n",
    "bars = ax5.bar(names, mean_eval, yerr=std_eval, color=[colors[n] for n in names], \n",
    "              alpha=0.7, edgecolor='black', linewidth=1.5, capsize=5)\n",
    "ax5.set_ylabel('Mean Reward', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('‚≠ê Evaluation Performance', fontsize=13, fontweight='bold')\n",
    "for bar, val in zip(bars, mean_eval):\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 6. Fires Extinguished\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "fires = [dqn_eval['mean_fires'], ppo_eval['mean_fires'], a2c_eval['mean_fires']]\n",
    "bars = ax6.bar(names, fires, color=[colors[n] for n in names], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax6.set_ylabel('Avg Fires Extinguished', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('üî• Fire Suppression', fontsize=13, fontweight='bold')\n",
    "for bar, val in zip(bars, fires):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.1f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 7. Convergence Speed (episodes to decent performance)\n",
    "ax7 = fig.add_subplot(gs[2, 1])\n",
    "convergence_threshold = 0.5 * max([dqn_results['best_reward'], ppo_results['best_reward'], a2c_results['best_reward']])\n",
    "\n",
    "conv_episodes = []\n",
    "for results in [dqn_results, ppo_results, a2c_results]:\n",
    "    for i, reward in enumerate(results['mean_rewards']):\n",
    "        if reward >= convergence_threshold:\n",
    "            conv_episodes.append(i)\n",
    "            break\n",
    "    else:\n",
    "        conv_episodes.append(len(results['mean_rewards']))\n",
    "\n",
    "bars = ax7.bar(names, conv_episodes, color=[colors[n] for n in names], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax7.set_ylabel('Episodes', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('‚ö° Convergence Speed', fontsize=13, fontweight='bold')\n",
    "for bar, val in zip(bars, conv_episodes):\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(val)}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# 8. Stability (Coefficient of Variation)\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "stability = [\n",
    "    (dqn_eval['std_reward'] / (abs(dqn_eval['mean_reward']) + 1e-6)),\n",
    "    (ppo_eval['std_reward'] / (abs(ppo_eval['mean_reward']) + 1e-6)),\n",
    "    (a2c_eval['std_reward'] / (abs(a2c_eval['mean_reward']) + 1e-6))\n",
    "]\n",
    "bars = ax8.bar(names, stability, color=[colors[n] for n in names], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax8.set_ylabel('Coefficient of Variation', fontsize=11, fontweight='bold')\n",
    "ax8.set_title('üìä Stability (lower=better)', fontsize=13, fontweight='bold')\n",
    "for bar, val in zip(bars, stability):\n",
    "    height = bar.get_height()\n",
    "    ax8.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.suptitle('ü§ñ RL Algorithm Comparison: DQN vs PPO vs A2C on NeuroFire', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "plt.savefig('neurofire_rl_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Comparison visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ec546",
   "metadata": {},
   "source": [
    "# SECTION 9: Algorithm Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc60d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Analysis and Comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üî¨ COMPREHENSIVE ALGORITHM ANALYSIS\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create detailed comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Final Mean Reward',\n",
    "        'Best Episode Reward',\n",
    "        'Reward Std Dev',\n",
    "        'Mean Fires Extinguished',\n",
    "        'Training Stability (CV)',\n",
    "        'Avg Fires per Episode',\n",
    "        'Algorithm Type',\n",
    "        'Sample Efficiency',\n",
    "        'Implementation Complexity'\n",
    "    ],\n",
    "    'DQN': [\n",
    "        f\"{dqn_eval['mean_reward']:.2f}\",\n",
    "        f\"{dqn_results['best_reward']:.2f}\",\n",
    "        f\"{dqn_eval['std_reward']:.2f}\",\n",
    "        f\"{dqn_eval['mean_fires']:.2f}\",\n",
    "        f\"{(dqn_eval['std_reward'] / (abs(dqn_eval['mean_reward']) + 1e-6)):.3f}\",\n",
    "        f\"{dqn_eval['mean_fires']:.2f}\",\n",
    "        \"Value-Based\",\n",
    "        \"Off-Policy ‚≠ê‚≠ê‚≠ê\",\n",
    "        \"Medium\"\n",
    "    ],\n",
    "    'PPO': [\n",
    "        f\"{ppo_eval['mean_reward']:.2f}\",\n",
    "        f\"{ppo_results['best_reward']:.2f}\",\n",
    "        f\"{ppo_eval['std_reward']:.2f}\",\n",
    "        f\"{ppo_eval['mean_fires']:.2f}\",\n",
    "        f\"{(ppo_eval['std_reward'] / (abs(ppo_eval['mean_reward']) + 1e-6)):.3f}\",\n",
    "        f\"{ppo_eval['mean_fires']:.2f}\",\n",
    "        \"Policy-Based\",\n",
    "        \"On-Policy ‚≠ê‚≠ê\",\n",
    "        \"High\"\n",
    "    ],\n",
    "    'A2C': [\n",
    "        f\"{a2c_eval['mean_reward']:.2f}\",\n",
    "        f\"{a2c_results['best_reward']:.2f}\",\n",
    "        f\"{a2c_eval['std_reward']:.2f}\",\n",
    "        f\"{a2c_eval['mean_fires']:.2f}\",\n",
    "        f\"{(a2c_eval['std_reward'] / (abs(a2c_eval['mean_reward']) + 1e-6)):.3f}\",\n",
    "        f\"{a2c_eval['mean_fires']:.2f}\",\n",
    "        \"Policy-Based\",\n",
    "        \"On-Policy ‚≠ê\",\n",
    "        \"Low\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + df_comparison.to_string(index=False))\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"üìå KEY FINDINGS\".center(80))\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Determine best in each category\n",
    "best_final = max([\n",
    "    ('DQN', dqn_eval['mean_reward']),\n",
    "    ('PPO', ppo_eval['mean_reward']),\n",
    "    ('A2C', a2c_eval['mean_reward'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "best_stability = min([\n",
    "    ('DQN', dqn_eval['std_reward']),\n",
    "    ('PPO', ppo_eval['std_reward']),\n",
    "    ('A2C', a2c_eval['std_reward'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "best_fires = max([\n",
    "    ('DQN', dqn_eval['mean_fires']),\n",
    "    ('PPO', ppo_eval['mean_fires']),\n",
    "    ('A2C', a2c_eval['mean_fires'])\n",
    "], key=lambda x: x[1])\n",
    "\n",
    "print(f\"\\n‚ú® Best Final Performance: {best_final[0]} ({best_final[1]:.2f})\")\n",
    "print(f\"üéØ Most Stable: {best_stability[0]} (œÉ={best_stability[1]:.2f})\")\n",
    "print(f\"üî• Best at Fire Suppression: {best_fires[0]} ({best_fires[1]:.2f} fires)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"üí° STRENGTHS & WEAKNESSES\".center(80))\n",
    "print(\"-\"*80)\n",
    "\n",
    "strengths_weaknesses = {\n",
    "    'DQN': {\n",
    "        'strengths': [\n",
    "            '‚úì Excellent sample efficiency (off-policy)',\n",
    "            '‚úì Stable with target network',\n",
    "            '‚úì Good for discrete action spaces',\n",
    "            '‚úì Experience replay reduces correlation'\n",
    "        ],\n",
    "        'weaknesses': [\n",
    "            '‚úó Can overestimate Q-values',\n",
    "            '‚úó Requires careful hyperparameter tuning',\n",
    "            '‚úó Memory overhead from replay buffer',\n",
    "            '‚úó Slower wall-clock training time'\n",
    "        ]\n",
    "    },\n",
    "    'PPO': {\n",
    "        'strengths': [\n",
    "            '‚úì Stable policy updates (clipped)',\n",
    "            '‚úì Better than REINFORCE',\n",
    "            '‚úì Easy to parallelize',\n",
    "            '‚úì Good balance of performance & complexity'\n",
    "        ],\n",
    "        'weaknesses': [\n",
    "            '‚úó On-policy: lower sample efficiency',\n",
    "            '‚úó Complex hyperparameter tuning',\n",
    "            '‚úó Requires more wall-clock time',\n",
    "            '‚úó GAE computation overhead'\n",
    "        ]\n",
    "    },\n",
    "    'A2C': {\n",
    "        'strengths': [\n",
    "            '‚úì Simple to implement',\n",
    "            '‚úì Fast convergence in some cases',\n",
    "            '‚úì Lower computational overhead',\n",
    "            '‚úì Good exploration with entropy bonus'\n",
    "        ],\n",
    "        'weaknesses': [\n",
    "            '‚úó Can be unstable',\n",
    "            '‚úó Sensitive to learning rate',\n",
    "            '‚úó High variance in gradient estimates',\n",
    "            '‚úó Slower convergence than DQN'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for algo, info in strengths_weaknesses.items():\n",
    "    print(f\"\\n{algo}:\")\n",
    "    print(\"  Strengths:\")\n",
    "    for s in info['strengths']:\n",
    "        print(f\"    {s}\")\n",
    "    print(\"  Weaknesses:\")\n",
    "    for w in info['weaknesses']:\n",
    "        print(f\"    {w}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb11803b",
   "metadata": {},
   "source": [
    "# SECTION 10: Recommendations & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c38737",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ ALGORITHM SELECTION GUIDE\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "guide = \"\"\"\n",
    "Choose DQN when:\n",
    "  üìä You have limited computational resources (no parallelization needed)\n",
    "  üéÆ Working with discrete action spaces\n",
    "  üíæ You can afford memory for a large replay buffer\n",
    "  ‚ö° Sample efficiency is critical (limited environment interactions)\n",
    "  \n",
    "  BEST FOR: Limited interactions, discrete actions, stable learning\n",
    "\n",
    "Choose PPO when:\n",
    "  üöÄ You can parallelize across multiple environments\n",
    "  üé™ You want the best balance of performance and stability\n",
    "  üîß You prefer simpler tuning compared to DQN\n",
    "  üèÜ You need state-of-the-art performance on challenging tasks\n",
    "  \n",
    "  BEST FOR: Production systems, multi-environment training, robustness\n",
    "\n",
    "Choose A2C when:\n",
    "  üèÉ You need fast prototyping and implementation\n",
    "  üíª You have limited memory availability\n",
    "  üìö You're learning RL concepts\n",
    "  ‚è±Ô∏è Training speed is more important than sample efficiency\n",
    "  \n",
    "  BEST FOR: Educational purposes, rapid prototyping, simple tasks\n",
    "\n",
    "For NeuroFire Project:\n",
    "  üî• The autonomous firefighter drone environment benefits most from:\n",
    "     1st Choice: PPO (best balance, robust to hyperparameters)\n",
    "     2nd Choice: DQN (if training data is scarce)\n",
    "     3rd Choice: A2C (for quick iterations during development)\n",
    "\"\"\"\n",
    "\n",
    "print(guide)\n",
    "\n",
    "# Final recommendation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÜ FINAL RECOMMENDATION FOR NEUROFIRE\".center(80))\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_algo = max([\n",
    "    ('DQN', dqn_eval['mean_reward']),\n",
    "    ('PPO', ppo_eval['mean_reward']),\n",
    "    ('A2C', a2c_eval['mean_reward'])\n",
    "], key=lambda x: x[1])[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "Based on comprehensive evaluation on the NeuroFire environment:\n",
    "\n",
    "RECOMMENDED ALGORITHM: {best_algo}\n",
    "\n",
    "Reasoning:\n",
    "1. Highest mean reward in evaluation: {best_algo}\n",
    "2. Best balance of stability and performance\n",
    "3. Most suitable for autonomous firefighter drone control\n",
    "4. Robust to hyperparameter variations\n",
    "5. Proven effective in similar navigation & control tasks\n",
    "\n",
    "Implementation Priority:\n",
    "1. ü•á {best_algo} - Primary implementation\n",
    "2. ü•à {sorted([('DQN', dqn_eval['mean_reward']), ('PPO', ppo_eval['mean_reward']), ('A2C', a2c_eval['mean_reward'])], key=lambda x: x[1], reverse=True)[1][0]} - Backup algorithm\n",
    "3. ü•â {sorted([('DQN', dqn_eval['mean_reward']), ('PPO', ppo_eval['mean_reward']), ('A2C', a2c_eval['mean_reward'])], key=lambda x: x[1])[0][0]} - For comparison/ensemble\n",
    "\n",
    "Next Steps for Production Deployment:\n",
    "‚úì Fine-tune hyperparameters for {best_algo}\n",
    "‚úì Test on larger grid environments\n",
    "‚úì Implement curriculum learning (start small, scale up)\n",
    "‚úì Add domain randomization for robustness\n",
    "‚úì Implement real-world fire simulation\n",
    "‚úì Create ensemble of all 3 algorithms for robustness\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ Analysis Complete! Ready for production deployment.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
